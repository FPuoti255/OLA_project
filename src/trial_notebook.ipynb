{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasciate questa cella cos√¨ ogni volta che aggiornate un file python, \n",
    "# anche gli import nel notebook vengono aggiornati\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Utils import *\n",
    "from constants import *\n",
    "\n",
    "from Environment import *\n",
    "from Non_Stationary_Environment import *\n",
    "\n",
    "from Social_influence import *\n",
    "from Network import Network\n",
    "\n",
    "from Simulation import *\n",
    "\n",
    "from Ecommerce import *\n",
    "from Ecommerce3 import *\n",
    "from Ecommerce4 import *\n",
    "from Ecommerce5 import *\n",
    "from Ecommerce6 import *\n",
    "from step7.Ecommerce7 import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal allocation is: [60. 60. 20. 60.  0.] with a reward of: 457\n",
      "estimated_opt_allocation is: [60. 60. 20. 60.  0.] with a reward of: 457\n"
     ]
    }
   ],
   "source": [
    "simulate_step2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpts_rewards_per_experiment, gpucb_rewards_per_experiment, opts = simulate_step3()\n",
    "plot_regrets(gpts_rewards_per_experiment, gpucb_rewards_per_experiment, opts, [\"GPTS\", \"GPUCB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpts_rewards_per_experiment, gpucb_rewards_per_experiment, opts = simulate_step4()\n",
    "plot_regrets(gpts_rewards_per_experiment, gpucb_rewards_per_experiment, opts, [\"GPTS\", \"GPUCB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpucb_rewards_per_experiment, gpts_rewards_per_experiment, opts = simulate_step5()\n",
    "plot_regrets(gpts_rewards_per_experiment, gpucb_rewards_per_experiment, opts, [\"GPTS\", \"GPUCB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swucb_rewards_per_experiment, cducb_rewards_per_experiment, opts = simulate_step6()\n",
    "plot_regrets(swucb_rewards_per_experiment, cducb_rewards_per_experiment, opts, [\"SWUCB\", \"CDUCB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env, observations_probabilities, click_probabilities, product_prices,\\\n",
    "    users_reservation_prices, users_poisson_parameters = generate_new_environment()\n",
    "\n",
    "\n",
    "nodes_activation_probabilities, num_sold_items = estimate_nodes_activation_probabilities(\n",
    "    click_probabilities,\n",
    "    users_reservation_prices,\n",
    "    users_poisson_parameters,\n",
    "    product_prices,\n",
    "    observations_probabilities\n",
    ")\n",
    "\n",
    "exp_num_clicks = env.estimate_disaggregated_num_clicks(budgets/B_cap)\n",
    "\n",
    "ecomm7 = Ecommerce7(B_cap, budgets, product_prices, features, 'TS')\n",
    "_, optimal_reward = ecomm7.clairvoyant_solve_optimization_problem(num_sold_items,\n",
    "                                                                  exp_num_clicks,\n",
    "                                                                  nodes_activation_probabilities)\n",
    "\n",
    "ts_gains_per_experiment = np.zeros(shape=T)\n",
    "\n",
    "for t in tqdm(range(0, T), position=0, desc=\"n_iteration\", leave=True):\n",
    "    context_learners = ecomm7.get_context_tree().get_leaves()\n",
    "    idxs = []\n",
    "    pulled_arms = np.zeros(shape=(NUM_OF_USERS_CLASSES, NUM_OF_PRODUCTS))\n",
    "\n",
    "    if t % split_time == 0 and t != 0:\n",
    "        for learner in context_learners:\n",
    "            learner.evaluate_splitting_condition(features,\n",
    "                                                 ecomm7.get_pulled_arms(\n",
    "                                                 )[-split_time:],\n",
    "                                                 ecomm7.get_collected_rewards(\n",
    "                                                 )[-split_time:],\n",
    "                                                 ecomm7.get_collected_sold_items()[-split_time:])\n",
    "\n",
    "        context_learners = ecomm7.get_context_tree().get_leaves()\n",
    "\n",
    "    \n",
    "    for learner in context_learners:\n",
    "        arm, idx = learner.pull_arm()\n",
    "        idxs.append(idx)\n",
    "        pulled_arms[idx, :] = arm\n",
    "\n",
    "    reward, estimated_sold_items = env.round_step7(\n",
    "        pulled_arms, B_cap, nodes_activation_probabilities, num_sold_items)\n",
    "\n",
    "    for i in range(len(context_learners)):\n",
    "        context_learners[i].update(\n",
    "            pulled_arms[idxs[i][0]],\n",
    "            np.sum(reward[idxs[i]], axis=0),\n",
    "            np.sum(estimated_sold_items[idxs[i]], axis=0)\n",
    "        )\n",
    "\n",
    "    ecomm7.update_history(pulled_arms, reward, estimated_sold_items)\n",
    "    for learner in context_learners:\n",
    "        _, rew = learner.algorithm.solve_optimization_problem(nodes_activation_probabilities)\n",
    "        ts_gains_per_experiment[t] += rew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1_regret = np.cumsum((optimal_reward-ts_gains_per_experiment))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10, 10))\n",
    "\n",
    "ticks = np.arange(start=1, stop=len(alg1_regret) + 1, step=1)\n",
    "\n",
    "ax[0].plot(ticks, alg1_regret, color='r')\n",
    "ax[0].set_xlabel('t')\n",
    "ax[0].set_ylabel('Regret')\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ola-project-8LfWcEjO-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "de57e29cc654ebd9fe415b8f06da5735bf250a61da6652d02ce0167dbae41596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
